{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "frequent_words = {}\n",
    "with open('freq17k.txt', 'r') as inf:\n",
    "    for line in inf:\n",
    "        frequent_words[line.strip().split('\\t')[0]] = 'not populated'\n",
    "        \n",
    "stopwords = []\n",
    "with open('stopwords.txt', 'r') as inf:\n",
    "    for line in inf:\n",
    "        stopwords.append(line.strip())\n",
    "\n",
    "with open('glove.6B.300d.txt', 'r') as inf:\n",
    "    for line in inf:\n",
    "        parts = line.strip().split(' ')\n",
    "        if parts[0] in frequent_words:\n",
    "            frequent_words[parts[0]] = np.array([float(x) for x in parts[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subendhu/anaconda/envs/allennlp/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "unp_keys = []\n",
    "for word in frequent_words:\n",
    "    if frequent_words[word] == 'not populated':\n",
    "        unp_keys.append(word)\n",
    "    \n",
    "for word in unp_keys:\n",
    "    del frequent_words[word]\n",
    "\n",
    "for word in stopwords:\n",
    "    if word in frequent_words:\n",
    "        del frequent_words[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: education\n",
      "Dimension: 276\n",
      "Closest words:\n",
      "('thousands', -3.0639)\n",
      "('residents', -3.0343)\n",
      "('palestinian', -3.0306)\n",
      "('hundreds', -3.0065)\n",
      "('police', -3.0031)\n",
      "('palestinians', -2.9903)\n",
      "('weapons', -2.9228)\n",
      "('students', -2.9186)\n",
      "('people', -2.9016)\n",
      "('use', -2.9015)\n"
     ]
    }
   ],
   "source": [
    "word = 'education'\n",
    "science_dims = np.abs(frequent_words[word])\n",
    "print('Word:', word)\n",
    "\n",
    "max_idx = np.argmax(science_dims)\n",
    "print('Dimension:', max_idx)\n",
    "\n",
    "closest_words = []\n",
    "for word in frequent_words:\n",
    "    closest_words.append((word, frequent_words[word][max_idx]))\n",
    "\n",
    "print('Closest words:')\n",
    "\n",
    "rev_flag = False\n",
    "if frequent_words[word][max_idx] > 0:\n",
    "    rev_flag = True\n",
    "closest_list = sorted(closest_words, key=lambda x: x[1], reverse=rev_flag)[:10]\n",
    "for i in closest_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "all_words = [(key, frequent_words[key]) for key in frequent_words.keys()]\n",
    "random.shuffle(all_words)\n",
    "train_set = all_words[:15000]\n",
    "test_set = all_words[15000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_words = [item[0] for item in train_set]\n",
    "train_vectors = [item[1] for item in train_set]\n",
    "\n",
    "test_words = [item[0] for item in test_set]\n",
    "test_vectors = [item[1] for item in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, sparse_dim=1000):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        self.hidden = nn.Linear(input_dim, sparse_dim)\n",
    "        self.out = nn.Linear(sparse_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + torch.normal(0, 0.4, size=x.size()).to(x.device)\n",
    "        h = torch.clamp(self.hidden(x), min=0, max=1)\n",
    "        o = self.out(h)\n",
    "        return o, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Step: 0 \t Training Loss: 0.5781896421873733\n",
      "Epoch: 0 \t Step: 100 \t Training Loss: 0.15287216749330187\n",
      "Epoch: 0 \t Step: 200 \t Training Loss: 0.14335025968848025\n",
      "Epoch: 0 \t Step: 300 \t Training Loss: 0.12329336006627403\n",
      "Epoch: 0 \t Step: 400 \t Training Loss: 0.11667178655735777\n",
      "Epoch: 0 \t Testing Reconstruction Loss: 0.09492387589848965\n",
      "Epoch: 1 \t Step: 0 \t Training Loss: 0.11350265327514808\n",
      "Epoch: 1 \t Step: 100 \t Training Loss: 0.10428688968389258\n",
      "Epoch: 1 \t Step: 200 \t Training Loss: 0.10275318759821532\n",
      "Epoch: 1 \t Step: 300 \t Training Loss: 0.09937993696813349\n",
      "Epoch: 1 \t Step: 400 \t Training Loss: 0.10286867977054837\n",
      "Epoch: 1 \t Testing Reconstruction Loss: 0.09238702697351153\n",
      "Epoch: 2 \t Step: 0 \t Training Loss: 0.09470932429806041\n",
      "Epoch: 2 \t Step: 100 \t Training Loss: 0.09417839725157937\n",
      "Epoch: 2 \t Step: 200 \t Training Loss: 0.09033812155398155\n",
      "Epoch: 2 \t Step: 300 \t Training Loss: 0.08759788667927318\n",
      "Epoch: 2 \t Step: 400 \t Training Loss: 0.08837546771283786\n",
      "Epoch: 2 \t Testing Reconstruction Loss: 0.09008863528596715\n",
      "Epoch: 3 \t Step: 0 \t Training Loss: 0.08784305960846572\n",
      "Epoch: 3 \t Step: 100 \t Training Loss: 0.09002360938288989\n",
      "Epoch: 3 \t Step: 200 \t Training Loss: 0.0831315427381067\n",
      "Epoch: 3 \t Step: 300 \t Training Loss: 0.08136591531466299\n",
      "Epoch: 3 \t Step: 400 \t Training Loss: 0.08187408926565021\n",
      "Epoch: 3 \t Testing Reconstruction Loss: 0.08913861203607487\n",
      "Epoch: 4 \t Step: 0 \t Training Loss: 0.08497363611396945\n",
      "Epoch: 4 \t Step: 100 \t Training Loss: 0.0764562794057093\n",
      "Epoch: 4 \t Step: 200 \t Training Loss: 0.0859884693503591\n",
      "Epoch: 4 \t Step: 300 \t Training Loss: 0.07572692633990731\n",
      "Epoch: 4 \t Step: 400 \t Training Loss: 0.07382966692276016\n",
      "Epoch: 4 \t Testing Reconstruction Loss: 0.08952293235202184\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = DenoisingAutoencoder(300).double()\n",
    "\n",
    "mse_criterion = nn.MSELoss()\n",
    "epochs = 5\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "sparsity_frac = 0.15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    steps = 0\n",
    "    random.shuffle(train_vectors)\n",
    "    net.train()\n",
    "    \n",
    "    for i in range(0, len(train_vectors), 32):\n",
    "        optimizer.zero_grad()\n",
    "        batch = np.array(train_vectors[i:i+32])\n",
    "        \n",
    "        inp = torch.from_numpy(batch).double()\n",
    "        out, hidden = net(inp)\n",
    "        target_sf = hidden.new_full(hidden[0].size(), fill_value=sparsity_frac)\n",
    "        \n",
    "        loss = mse_criterion(out, inp) + torch.sum(torch.clamp((torch.mean(hidden, axis=0) - target_sf), min=0) ** 2) + torch.mean(hidden * (1 - hidden))\n",
    "        loss.backward()\n",
    "        if steps % 100 == 0:\n",
    "            print('Epoch: {} \\t Step: {} \\t Training Loss: {}'.format(epoch, steps, loss.detach().numpy()))\n",
    "        steps += 1\n",
    "        optimizer.step()\n",
    "        \n",
    "    net.eval()\n",
    "    batch = np.array(test_vectors)\n",
    "    noise = np.random.normal(0, 0.4, batch.shape)\n",
    "    inp = torch.from_numpy(batch).double()\n",
    "    noisy_inp = torch.from_numpy(batch + noise).double()\n",
    "    out, hidden = net(noisy_inp)\n",
    "    loss = mse_criterion(out, inp)\n",
    "    print('Epoch: {} \\t Testing Reconstruction Loss: {}'.format(epoch, loss.detach().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "sparse_vectors = {}\n",
    "\n",
    "for word in frequent_words:\n",
    "    o, h = net(torch.from_numpy(np.array([frequent_words[word]])))\n",
    "    sparse_vectors[word] = h.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: education\n",
      "Dimension: 237\n",
      "Closest words:\n",
      "('school', 1.0)\n",
      "('university', 1.0)\n",
      "('education', 1.0)\n",
      "('students', 1.0)\n",
      "('college', 1.0)\n",
      "('science', 1.0)\n",
      "('student', 1.0)\n",
      "('schools', 1.0)\n",
      "('studies', 1.0)\n",
      "('engineering', 1.0)\n"
     ]
    }
   ],
   "source": [
    "word = 'education'\n",
    "science_dims = np.abs(sparse_vectors[word])\n",
    "print('Word:', word)\n",
    "\n",
    "max_idx = np.argmax(science_dims)\n",
    "print('Dimension:', max_idx)\n",
    "\n",
    "closest_words = []\n",
    "for word in sparse_vectors:\n",
    "    closest_words.append((word, abs(sparse_vectors[word][max_idx])))\n",
    "\n",
    "print('Closest words:')    \n",
    "closest_list = sorted(closest_words, key=lambda x: x[1], reverse=True)[:10]\n",
    "for i in closest_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "model = BertModel.from_pretrained(pretrained_weights)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16690/16690 [15:35<00:00, 17.83it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "bert_vectors = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for word in tqdm(frequent_words):\n",
    "        input_ids = torch.tensor([tokenizer.encode(word)])\n",
    "        bert_vectors[word] = model(input_ids)[0][:,1,:].detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "all_vectors = []\n",
    "for word in bert_vectors:\n",
    "    all_vectors.append(bert_vectors[word])\n",
    "all_vectors = np.array(all_vectors)\n",
    "\n",
    "min_av = np.min(all_vectors, axis=0)\n",
    "max_av = np.max(all_vectors, axis=0)\n",
    "\n",
    "norm_bert_vectors = {}\n",
    "for word in bert_vectors:\n",
    "    norm_bert_vectors[word] = ((bert_vectors[word] - min_av) / (max_av - min_av))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: science\n",
      "Dimension: 308\n",
      "Closest words:\n",
      "('lifted', -6.2681575)\n",
      "('servings', -6.258731)\n",
      "('intervals', -6.249471)\n",
      "('decks', -6.235012)\n",
      "('advisors', -6.227306)\n",
      "('beverage', -6.1765027)\n",
      "('lendingtree', -6.172181)\n",
      "('empires', -6.172099)\n",
      "('funeral', -6.159995)\n",
      "('malls', -6.1489334)\n"
     ]
    }
   ],
   "source": [
    "word = 'science'\n",
    "science_dims = np.abs(bert_vectors[word])\n",
    "print('Word:', word)\n",
    "\n",
    "max_idx = np.argmax(science_dims)\n",
    "print('Dimension:', max_idx)\n",
    "\n",
    "closest_words = []\n",
    "for word in bert_vectors:\n",
    "    closest_words.append((word, bert_vectors[word][max_idx]))\n",
    "\n",
    "print('Closest words:') \n",
    "rev_flag = False\n",
    "if bert_vectors[word][max_idx] > 0:\n",
    "    rev_flag = True\n",
    "closest_list = sorted(closest_words, key=lambda x: x[1], reverse=rev_flag)[:10]\n",
    "for i in closest_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Step: 0 \t Training Loss: 24.705243692891706\n",
      "Epoch: 0 \t Step: 100 \t Training Loss: 0.21230462847440645\n",
      "Epoch: 0 \t Step: 200 \t Training Loss: 0.19957984836353118\n",
      "Epoch: 0 \t Step: 300 \t Training Loss: 0.18407323390696032\n",
      "Epoch: 0 \t Step: 400 \t Training Loss: 0.16722655797341202\n",
      "Epoch: 0 \t Testing Reconstruction Loss: 0.15173368302568643\n",
      "Epoch: 1 \t Step: 0 \t Training Loss: 0.18502710552696888\n",
      "Epoch: 1 \t Step: 100 \t Training Loss: 0.1663344528581696\n",
      "Epoch: 1 \t Step: 200 \t Training Loss: 0.13742076275933376\n",
      "Epoch: 1 \t Step: 300 \t Training Loss: 0.1536622593574052\n",
      "Epoch: 1 \t Step: 400 \t Training Loss: 0.13926119860651148\n",
      "Epoch: 1 \t Testing Reconstruction Loss: 0.12867589656729933\n",
      "Epoch: 2 \t Step: 0 \t Training Loss: 0.1442479467894162\n",
      "Epoch: 2 \t Step: 100 \t Training Loss: 0.11962366413991951\n",
      "Epoch: 2 \t Step: 200 \t Training Loss: 0.13099387691161576\n",
      "Epoch: 2 \t Step: 300 \t Training Loss: 0.11106738546584377\n",
      "Epoch: 2 \t Step: 400 \t Training Loss: 0.11353824687654704\n",
      "Epoch: 2 \t Testing Reconstruction Loss: 0.11860809399134793\n",
      "Epoch: 3 \t Step: 0 \t Training Loss: 0.12441573853506108\n",
      "Epoch: 3 \t Step: 100 \t Training Loss: 0.1329978522310652\n",
      "Epoch: 3 \t Step: 200 \t Training Loss: 0.1192739234712909\n",
      "Epoch: 3 \t Step: 300 \t Training Loss: 0.11147338971901313\n",
      "Epoch: 3 \t Step: 400 \t Training Loss: 0.10905630579786936\n",
      "Epoch: 3 \t Testing Reconstruction Loss: 0.11362620987366537\n",
      "Epoch: 4 \t Step: 0 \t Training Loss: 0.11997778978767171\n",
      "Epoch: 4 \t Step: 100 \t Training Loss: 0.10632610356092781\n",
      "Epoch: 4 \t Step: 200 \t Training Loss: 0.10243432277752812\n",
      "Epoch: 4 \t Step: 300 \t Training Loss: 0.12115765591430334\n",
      "Epoch: 4 \t Step: 400 \t Training Loss: 0.1438826121777502\n",
      "Epoch: 4 \t Testing Reconstruction Loss: 0.10873993995440218\n",
      "Epoch: 5 \t Step: 0 \t Training Loss: 0.11074270601508353\n",
      "Epoch: 5 \t Step: 100 \t Training Loss: 0.09452229322203909\n",
      "Epoch: 5 \t Step: 200 \t Training Loss: 0.12257107160041364\n",
      "Epoch: 5 \t Step: 300 \t Training Loss: 0.11662310416328601\n",
      "Epoch: 5 \t Step: 400 \t Training Loss: 0.11060666353829761\n",
      "Epoch: 5 \t Testing Reconstruction Loss: 0.1067429763253562\n",
      "Epoch: 6 \t Step: 0 \t Training Loss: 0.10163348122675404\n",
      "Epoch: 6 \t Step: 100 \t Training Loss: 0.11816677215171507\n",
      "Epoch: 6 \t Step: 200 \t Training Loss: 0.11113212640334655\n",
      "Epoch: 6 \t Step: 300 \t Training Loss: 0.10994069922087141\n",
      "Epoch: 6 \t Step: 400 \t Training Loss: 0.10885145704166939\n",
      "Epoch: 6 \t Testing Reconstruction Loss: 0.1045208932501214\n",
      "Epoch: 7 \t Step: 0 \t Training Loss: 0.10387567041528284\n",
      "Epoch: 7 \t Step: 100 \t Training Loss: 0.09943657488181036\n",
      "Epoch: 7 \t Step: 200 \t Training Loss: 0.09622373053934521\n",
      "Epoch: 7 \t Step: 300 \t Training Loss: 0.10555393684285538\n",
      "Epoch: 7 \t Step: 400 \t Training Loss: 0.12035556761235687\n",
      "Epoch: 7 \t Testing Reconstruction Loss: 0.10334733956147583\n",
      "Epoch: 8 \t Step: 0 \t Training Loss: 0.105303056874482\n",
      "Epoch: 8 \t Step: 100 \t Training Loss: 0.10952174142256023\n",
      "Epoch: 8 \t Step: 200 \t Training Loss: 0.10023625463856609\n",
      "Epoch: 8 \t Step: 300 \t Training Loss: 0.10975550140341143\n",
      "Epoch: 8 \t Step: 400 \t Training Loss: 0.09244743881413822\n",
      "Epoch: 8 \t Testing Reconstruction Loss: 0.09947330851293255\n",
      "Epoch: 9 \t Step: 0 \t Training Loss: 0.08234350000526261\n",
      "Epoch: 9 \t Step: 100 \t Training Loss: 0.0882771331542328\n",
      "Epoch: 9 \t Step: 200 \t Training Loss: 0.08981586985374222\n",
      "Epoch: 9 \t Step: 300 \t Training Loss: 0.09075173062467978\n",
      "Epoch: 9 \t Step: 400 \t Training Loss: 0.11241548835836782\n",
      "Epoch: 9 \t Testing Reconstruction Loss: 0.09975195427069754\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "all_words = [(key, bert_vectors[key]) for key in bert_vectors.keys()]\n",
    "random.shuffle(all_words)\n",
    "train_set = all_words[:15000]\n",
    "test_set = all_words[15000:]\n",
    "\n",
    "train_words = [item[0] for item in train_set]\n",
    "train_vectors = [item[1] for item in train_set]\n",
    "\n",
    "test_words = [item[0] for item in test_set]\n",
    "test_vectors = [item[1] for item in test_set]\n",
    "\n",
    "bert_net = DenoisingAutoencoder(768).double()\n",
    "\n",
    "mse_criterion = nn.MSELoss()\n",
    "epochs = 10\n",
    "optimizer = optim.Adam(bert_net.parameters())\n",
    "sparsity_frac = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    steps = 0\n",
    "    random.shuffle(train_vectors)\n",
    "    bert_net.train()\n",
    "    \n",
    "    for i in range(0, len(train_vectors), 32):\n",
    "        optimizer.zero_grad()\n",
    "        batch = np.array(train_vectors[i:i+32])\n",
    "        \n",
    "        inp = torch.from_numpy(batch).double()\n",
    "        out, hidden = bert_net(inp)\n",
    "        target_sf = hidden.new_full(hidden[0].size(), fill_value=sparsity_frac)\n",
    "        \n",
    "        loss = mse_criterion(out, inp) + torch.sum(torch.clamp((torch.mean(hidden, axis=0) - target_sf), min=0) ** 2) + torch.mean(hidden * (1 - hidden))\n",
    "        loss.backward()\n",
    "        if steps % 100 == 0:\n",
    "            print('Epoch: {} \\t Step: {} \\t Training Loss: {}'.format(epoch, steps, loss.detach().numpy()))\n",
    "        steps += 1\n",
    "        optimizer.step()\n",
    "        \n",
    "    bert_net.eval()\n",
    "    batch = np.array(test_vectors)\n",
    "    inp = torch.from_numpy(batch).double()\n",
    "    out, hidden = bert_net(inp)\n",
    "    loss = mse_criterion(out, inp)\n",
    "    print('Epoch: {} \\t Testing Reconstruction Loss: {}'.format(epoch, loss.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_net.eval()\n",
    "sparse_bert_vectors = {}\n",
    "\n",
    "for word in frequent_words:\n",
    "    o, h = bert_net(torch.from_numpy(np.array([bert_vectors[word]])).double())\n",
    "    sparse_bert_vectors[word] = h.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: science\n",
      "Dimension: 925\n",
      "Closest words:\n",
      "('business', 1.0)\n",
      "('health', 1.0)\n",
      "('policy', 1.0)\n",
      "('research', 1.0)\n",
      "('management', 1.0)\n",
      "('development', 1.0)\n",
      "('education', 1.0)\n",
      "('design', 1.0)\n",
      "('computer', 1.0)\n",
      "('systems', 1.0)\n"
     ]
    }
   ],
   "source": [
    "word = 'science'\n",
    "science_dims = np.abs(sparse_bert_vectors[word])\n",
    "print('Word:', word)\n",
    "\n",
    "max_idx = np.argmax(science_dims)\n",
    "print('Dimension:', max_idx)\n",
    "\n",
    "closest_words = []\n",
    "for word in bert_vectors:\n",
    "    closest_words.append((word, abs(sparse_bert_vectors[word][max_idx])))\n",
    "\n",
    "print('Closest words:')    \n",
    "closest_list = sorted(closest_words, key=lambda x: x[1], reverse=True)[:10]\n",
    "for i in closest_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(bert_net.state_dict(), 'sparse_net_bert.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from transformers import BertModel, BertPreTrainedModel, BertConfig \n",
    "\n",
    "class BertForSequenceClassificationWithSparsity(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.sparse_size = 1000\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "        \n",
    "        self.sparse_net = DenoisingAutoencoder(config.hidden_size, self.sparse_size)\n",
    "        self.sparse_net.load_state_dict(torch.load('sparse_net_bert.pth'))\n",
    "        self.sparsity_frac = 0.05\n",
    "        self.sparsity_imp = 0.1\n",
    "        \n",
    "        self.sparse_dense = nn.Linear(self.sparse_size, self.sparse_size)\n",
    "        self.sparse_activation = nn.Tanh()\n",
    "        self.sparse_classifier = nn.Linear(self.sparse_size, self.config.num_labels)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        osize = outputs[0].size()\n",
    "        all_outputs = outputs[0].reshape(osize[0]*osize[1], self.hidden_size)\n",
    "        rec_outputs, sparse_outputs = self.sparse_net(all_outputs)\n",
    "        \n",
    "#         pooled_output = outputs[1]\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss_recon = MSELoss()\n",
    "        target_sf = sparse_outputs.new_full(sparse_outputs[0].size(), fill_value=self.sparsity_frac)\n",
    "        loss = self.sparsity_imp * (loss_recon(rec_outputs, all_outputs) + torch.sum(torch.clamp((sparse_outputs.mean(axis=0) - target_sf), min=0) ** 2) + torch.mean(sparse_outputs * (1 - sparse_outputs)))\n",
    "        \n",
    "        sparse_outputs = sparse_outputs.reshape(osize[0], osize[1], -1)\n",
    "        sp_first_token_tensor = sparse_outputs[:, 0]\n",
    "        sp_pooled_output = self.sparse_dense(sp_first_token_tensor)\n",
    "        sp_pooled_output = self.sparse_activation(sp_pooled_output)\n",
    "        sp_pooled_output = self.dropout(sp_pooled_output)\n",
    "        sp_logits = self.sparse_classifier(sp_pooled_output)\n",
    "        \n",
    "        outputs = (sp_logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        \n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss += loss_fct(sp_logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss += loss_fct(sp_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-uncased',\n",
    "                                      num_labels=3,\n",
    "                                      finetuning_task='xnli',\n",
    "                                      cache_dir=None)\n",
    "\n",
    "model = BertForSequenceClassificationWithSparsity.from_pretrained('bert-base-uncased',\n",
    "                                        from_tf=False,\n",
    "                                        config=config,\n",
    "                                        cache_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode('this is it'), tokenizer.encode('this is not')])\n",
    "labels = torch.tensor([0, 1])\n",
    "a, b = model(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
