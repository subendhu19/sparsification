{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "frequent_words = {}\n",
    "with open('freq17k.txt', 'r') as inf:\n",
    "    for line in inf:\n",
    "        frequent_words[line.strip().split('\\t')[0]] = 'not populated'\n",
    "        \n",
    "stopwords = []\n",
    "with open('stopwords.txt', 'r') as inf:\n",
    "    for line in inf:\n",
    "        stopwords.append(line.strip())\n",
    "\n",
    "with open('glove.6B.300d.txt', 'r') as inf:\n",
    "    for line in inf:\n",
    "        parts = line.strip().split(' ')\n",
    "        if parts[0] in frequent_words:\n",
    "            frequent_words[parts[0]] = np.array([float(x) for x in parts[1:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/subendhu/anaconda/envs/allennlp/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "unp_keys = []\n",
    "for word in frequent_words:\n",
    "    if frequent_words[word] == 'not populated':\n",
    "        unp_keys.append(word)\n",
    "    \n",
    "for word in unp_keys:\n",
    "    del frequent_words[word]\n",
    "\n",
    "for word in stopwords:\n",
    "    if word in frequent_words:\n",
    "        del frequent_words[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: education\n",
      "Dimension: 276\n",
      "Closest words:\n",
      "('thousands', -3.0639)\n",
      "('residents', -3.0343)\n",
      "('palestinian', -3.0306)\n",
      "('hundreds', -3.0065)\n",
      "('police', -3.0031)\n",
      "('palestinians', -2.9903)\n",
      "('weapons', -2.9228)\n",
      "('students', -2.9186)\n",
      "('people', -2.9016)\n",
      "('use', -2.9015)\n"
     ]
    }
   ],
   "source": [
    "word = 'education'\n",
    "science_dims = np.abs(frequent_words[word])\n",
    "print('Word:', word)\n",
    "\n",
    "max_idx = np.argmax(science_dims)\n",
    "print('Dimension:', max_idx)\n",
    "\n",
    "closest_words = []\n",
    "for word in frequent_words:\n",
    "    closest_words.append((word, frequent_words[word][max_idx]))\n",
    "\n",
    "print('Closest words:')\n",
    "\n",
    "rev_flag = False\n",
    "if frequent_words[word][max_idx] > 0:\n",
    "    rev_flag = True\n",
    "closest_list = sorted(closest_words, key=lambda x: x[1], reverse=rev_flag)[:10]\n",
    "for i in closest_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "all_words = [(key, frequent_words[key]) for key in frequent_words.keys()]\n",
    "random.shuffle(all_words)\n",
    "train_set = all_words[:15000]\n",
    "test_set = all_words[15000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_words = [item[0] for item in train_set]\n",
    "train_vectors = [item[1] for item in train_set]\n",
    "\n",
    "test_words = [item[0] for item in test_set]\n",
    "test_vectors = [item[1] for item in test_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, sparse_dim=1000):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        self.hidden = nn.Linear(input_dim, sparse_dim)\n",
    "        self.out = nn.Linear(sparse_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + torch.normal(0, 0.4, size=x.size()).to(x.device)\n",
    "        h = torch.clamp(self.hidden(x), min=0, max=1)\n",
    "        o = self.out(h)\n",
    "        return o, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Step: 0 \t Training Loss: 0.5781896421873733\n",
      "Epoch: 0 \t Step: 100 \t Training Loss: 0.15287216749330187\n",
      "Epoch: 0 \t Step: 200 \t Training Loss: 0.14335025968848025\n",
      "Epoch: 0 \t Step: 300 \t Training Loss: 0.12329336006627403\n",
      "Epoch: 0 \t Step: 400 \t Training Loss: 0.11667178655735777\n",
      "Epoch: 0 \t Testing Reconstruction Loss: 0.09492387589848965\n",
      "Epoch: 1 \t Step: 0 \t Training Loss: 0.11350265327514808\n",
      "Epoch: 1 \t Step: 100 \t Training Loss: 0.10428688968389258\n",
      "Epoch: 1 \t Step: 200 \t Training Loss: 0.10275318759821532\n",
      "Epoch: 1 \t Step: 300 \t Training Loss: 0.09937993696813349\n",
      "Epoch: 1 \t Step: 400 \t Training Loss: 0.10286867977054837\n",
      "Epoch: 1 \t Testing Reconstruction Loss: 0.09238702697351153\n",
      "Epoch: 2 \t Step: 0 \t Training Loss: 0.09470932429806041\n",
      "Epoch: 2 \t Step: 100 \t Training Loss: 0.09417839725157937\n",
      "Epoch: 2 \t Step: 200 \t Training Loss: 0.09033812155398155\n",
      "Epoch: 2 \t Step: 300 \t Training Loss: 0.08759788667927318\n",
      "Epoch: 2 \t Step: 400 \t Training Loss: 0.08837546771283786\n",
      "Epoch: 2 \t Testing Reconstruction Loss: 0.09008863528596715\n",
      "Epoch: 3 \t Step: 0 \t Training Loss: 0.08784305960846572\n",
      "Epoch: 3 \t Step: 100 \t Training Loss: 0.09002360938288989\n",
      "Epoch: 3 \t Step: 200 \t Training Loss: 0.0831315427381067\n",
      "Epoch: 3 \t Step: 300 \t Training Loss: 0.08136591531466299\n",
      "Epoch: 3 \t Step: 400 \t Training Loss: 0.08187408926565021\n",
      "Epoch: 3 \t Testing Reconstruction Loss: 0.08913861203607487\n",
      "Epoch: 4 \t Step: 0 \t Training Loss: 0.08497363611396945\n",
      "Epoch: 4 \t Step: 100 \t Training Loss: 0.0764562794057093\n",
      "Epoch: 4 \t Step: 200 \t Training Loss: 0.0859884693503591\n",
      "Epoch: 4 \t Step: 300 \t Training Loss: 0.07572692633990731\n",
      "Epoch: 4 \t Step: 400 \t Training Loss: 0.07382966692276016\n",
      "Epoch: 4 \t Testing Reconstruction Loss: 0.08952293235202184\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "net = DenoisingAutoencoder(300).double()\n",
    "\n",
    "mse_criterion = nn.MSELoss()\n",
    "epochs = 5\n",
    "optimizer = optim.Adam(net.parameters())\n",
    "sparsity_frac = 0.15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    steps = 0\n",
    "    random.shuffle(train_vectors)\n",
    "    net.train()\n",
    "    \n",
    "    for i in range(0, len(train_vectors), 32):\n",
    "        optimizer.zero_grad()\n",
    "        batch = np.array(train_vectors[i:i+32])\n",
    "        \n",
    "        inp = torch.from_numpy(batch).double()\n",
    "        out, hidden = net(inp)\n",
    "        target_sf = hidden.new_full(hidden[0].size(), fill_value=sparsity_frac)\n",
    "        \n",
    "        loss = mse_criterion(out, inp) + torch.sum(torch.clamp((torch.mean(hidden, axis=0) - target_sf), min=0) ** 2) + torch.mean(hidden * (1 - hidden))\n",
    "        loss.backward()\n",
    "        if steps % 100 == 0:\n",
    "            print('Epoch: {} \\t Step: {} \\t Training Loss: {}'.format(epoch, steps, loss.detach().numpy()))\n",
    "        steps += 1\n",
    "        optimizer.step()\n",
    "        \n",
    "    net.eval()\n",
    "    batch = np.array(test_vectors)\n",
    "    noise = np.random.normal(0, 0.4, batch.shape)\n",
    "    inp = torch.from_numpy(batch).double()\n",
    "    noisy_inp = torch.from_numpy(batch + noise).double()\n",
    "    out, hidden = net(noisy_inp)\n",
    "    loss = mse_criterion(out, inp)\n",
    "    print('Epoch: {} \\t Testing Reconstruction Loss: {}'.format(epoch, loss.detach().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.eval()\n",
    "sparse_vectors = {}\n",
    "\n",
    "for word in frequent_words:\n",
    "    o, h = net(torch.from_numpy(np.array([frequent_words[word]])))\n",
    "    sparse_vectors[word] = h.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: education\n",
      "Dimension: 237\n",
      "Closest words:\n",
      "('school', 1.0)\n",
      "('university', 1.0)\n",
      "('education', 1.0)\n",
      "('students', 1.0)\n",
      "('college', 1.0)\n",
      "('science', 1.0)\n",
      "('student', 1.0)\n",
      "('schools', 1.0)\n",
      "('studies', 1.0)\n",
      "('engineering', 1.0)\n"
     ]
    }
   ],
   "source": [
    "word = 'education'\n",
    "science_dims = np.abs(sparse_vectors[word])\n",
    "print('Word:', word)\n",
    "\n",
    "max_idx = np.argmax(science_dims)\n",
    "print('Dimension:', max_idx)\n",
    "\n",
    "closest_words = []\n",
    "for word in sparse_vectors:\n",
    "    closest_words.append((word, abs(sparse_vectors[word][max_idx])))\n",
    "\n",
    "print('Closest words:')    \n",
    "closest_list = sorted(closest_words, key=lambda x: x[1], reverse=True)[:10]\n",
    "for i in closest_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "pretrained_weights = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n",
    "model = BertModel.from_pretrained(pretrained_weights)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16690/16690 [15:55<00:00, 17.46it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "bert_vectors = {}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for word in tqdm(frequent_words):\n",
    "        input_ids = torch.tensor([tokenizer.encode(word)])\n",
    "        bert_vectors[word] = model(input_ids)[0][:,1,:].detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalization\n",
    "\n",
    "all_vectors = []\n",
    "for word in bert_vectors:\n",
    "    all_vectors.append(bert_vectors[word])\n",
    "all_vectors = np.array(all_vectors)\n",
    "\n",
    "min_av = np.min(all_vectors, axis=0)\n",
    "max_av = np.max(all_vectors, axis=0)\n",
    "\n",
    "norm_bert_vectors = {}\n",
    "for word in bert_vectors:\n",
    "    norm_bert_vectors[word] = ((bert_vectors[word] - min_av) / (max_av - min_av))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: science\n",
      "Dimension: 308\n",
      "Closest words:\n",
      "('lifted', -6.2681575)\n",
      "('servings', -6.258731)\n",
      "('intervals', -6.249471)\n",
      "('decks', -6.235012)\n",
      "('advisors', -6.227306)\n",
      "('beverage', -6.1765027)\n",
      "('lendingtree', -6.172181)\n",
      "('empires', -6.172099)\n",
      "('funeral', -6.159995)\n",
      "('malls', -6.1489334)\n"
     ]
    }
   ],
   "source": [
    "word = 'science'\n",
    "science_dims = np.abs(bert_vectors[word])\n",
    "print('Word:', word)\n",
    "\n",
    "max_idx = np.argmax(science_dims)\n",
    "print('Dimension:', max_idx)\n",
    "\n",
    "closest_words = []\n",
    "for word in bert_vectors:\n",
    "    closest_words.append((word, bert_vectors[word][max_idx]))\n",
    "\n",
    "print('Closest words:') \n",
    "rev_flag = False\n",
    "if bert_vectors[word][max_idx] > 0:\n",
    "    rev_flag = True\n",
    "closest_list = sorted(closest_words, key=lambda x: x[1], reverse=rev_flag)[:10]\n",
    "for i in closest_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \t Step: 0 \t Training Loss: 34.82261159201152\n",
      "Epoch: 0 \t Step: 100 \t Training Loss: 0.22098053312640317\n",
      "Epoch: 0 \t Step: 200 \t Training Loss: 0.1980836235463653\n",
      "Epoch: 0 \t Step: 300 \t Training Loss: 0.1732385120508834\n",
      "Epoch: 0 \t Step: 400 \t Training Loss: 0.15348737330110887\n",
      "Epoch: 0 \t Testing Reconstruction Loss: 0.15339409885312652\n",
      "Epoch: 1 \t Step: 0 \t Training Loss: 0.1515732235327169\n",
      "Epoch: 1 \t Step: 100 \t Training Loss: 0.17068721398301068\n",
      "Epoch: 1 \t Step: 200 \t Training Loss: 0.14867206741991323\n",
      "Epoch: 1 \t Step: 300 \t Training Loss: 0.12990709795930677\n",
      "Epoch: 1 \t Step: 400 \t Training Loss: 0.14331825219043892\n",
      "Epoch: 1 \t Testing Reconstruction Loss: 0.12684075475239034\n",
      "Epoch: 2 \t Step: 0 \t Training Loss: 0.12936777289950485\n",
      "Epoch: 2 \t Step: 100 \t Training Loss: 0.13666537633099768\n",
      "Epoch: 2 \t Step: 200 \t Training Loss: 0.12641225766798894\n",
      "Epoch: 2 \t Step: 300 \t Training Loss: 0.11535100102314302\n",
      "Epoch: 2 \t Step: 400 \t Training Loss: 0.13335425129276854\n",
      "Epoch: 2 \t Testing Reconstruction Loss: 0.1168894576632772\n",
      "Epoch: 3 \t Step: 0 \t Training Loss: 0.12723718236892587\n",
      "Epoch: 3 \t Step: 100 \t Training Loss: 0.11204977180718051\n",
      "Epoch: 3 \t Step: 200 \t Training Loss: 0.10390178063063886\n",
      "Epoch: 3 \t Step: 300 \t Training Loss: 0.1179781745294492\n",
      "Epoch: 3 \t Step: 400 \t Training Loss: 0.11217026528721023\n",
      "Epoch: 3 \t Testing Reconstruction Loss: 0.11014640025652629\n",
      "Epoch: 4 \t Step: 0 \t Training Loss: 0.09943812398291485\n",
      "Epoch: 4 \t Step: 100 \t Training Loss: 0.12381174946392541\n",
      "Epoch: 4 \t Step: 200 \t Training Loss: 0.1031937743466025\n",
      "Epoch: 4 \t Step: 300 \t Training Loss: 0.11379051272266465\n",
      "Epoch: 4 \t Step: 400 \t Training Loss: 0.12195305671386261\n",
      "Epoch: 4 \t Testing Reconstruction Loss: 0.10501717388055991\n",
      "Epoch: 5 \t Step: 0 \t Training Loss: 0.09947930705237408\n",
      "Epoch: 5 \t Step: 100 \t Training Loss: 0.10496547081324949\n",
      "Epoch: 5 \t Step: 200 \t Training Loss: 0.09388526433767379\n",
      "Epoch: 5 \t Step: 300 \t Training Loss: 0.09927824409280916\n",
      "Epoch: 5 \t Step: 400 \t Training Loss: 0.10033105483994419\n",
      "Epoch: 5 \t Testing Reconstruction Loss: 0.10500407403837307\n",
      "Epoch: 6 \t Step: 0 \t Training Loss: 0.10183374352807713\n",
      "Epoch: 6 \t Step: 100 \t Training Loss: 0.11001529648462677\n",
      "Epoch: 6 \t Step: 200 \t Training Loss: 0.10282617919141847\n",
      "Epoch: 6 \t Step: 300 \t Training Loss: 0.09511713143399135\n",
      "Epoch: 6 \t Step: 400 \t Training Loss: 0.099252301114367\n",
      "Epoch: 6 \t Testing Reconstruction Loss: 0.10151780324562978\n",
      "Epoch: 7 \t Step: 0 \t Training Loss: 0.09492443039537675\n",
      "Epoch: 7 \t Step: 100 \t Training Loss: 0.10157420453897198\n",
      "Epoch: 7 \t Step: 200 \t Training Loss: 0.09804462250071747\n",
      "Epoch: 7 \t Step: 300 \t Training Loss: 0.10168081479039508\n",
      "Epoch: 7 \t Step: 400 \t Training Loss: 0.10166665044634667\n",
      "Epoch: 7 \t Testing Reconstruction Loss: 0.09958679141683649\n",
      "Epoch: 8 \t Step: 0 \t Training Loss: 0.10989752478873319\n",
      "Epoch: 8 \t Step: 100 \t Training Loss: 0.10139286325916373\n",
      "Epoch: 8 \t Step: 200 \t Training Loss: 0.10139659415536645\n",
      "Epoch: 8 \t Step: 300 \t Training Loss: 0.1090483843840372\n",
      "Epoch: 8 \t Step: 400 \t Training Loss: 0.09901433518803157\n",
      "Epoch: 8 \t Testing Reconstruction Loss: 0.1002361298328486\n",
      "Epoch: 9 \t Step: 0 \t Training Loss: 0.11923943883921574\n",
      "Epoch: 9 \t Step: 100 \t Training Loss: 0.08258172528334692\n",
      "Epoch: 9 \t Step: 200 \t Training Loss: 0.08943575871937384\n",
      "Epoch: 9 \t Step: 300 \t Training Loss: 0.10856607207237216\n",
      "Epoch: 9 \t Step: 400 \t Training Loss: 0.09204203445097228\n",
      "Epoch: 9 \t Testing Reconstruction Loss: 0.09772076245695302\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch.optim as optim\n",
    "\n",
    "all_words = [(key, bert_vectors[key]) for key in bert_vectors.keys()]\n",
    "random.shuffle(all_words)\n",
    "train_set = all_words[:15000]\n",
    "test_set = all_words[15000:]\n",
    "\n",
    "train_words = [item[0] for item in train_set]\n",
    "train_vectors = [item[1] for item in train_set]\n",
    "\n",
    "test_words = [item[0] for item in test_set]\n",
    "test_vectors = [item[1] for item in test_set]\n",
    "\n",
    "bert_net = DenoisingAutoencoder(768, 1500).double()\n",
    "\n",
    "mse_criterion = nn.MSELoss()\n",
    "epochs = 10\n",
    "optimizer = optim.Adam(bert_net.parameters())\n",
    "sparsity_frac = 0.05\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    steps = 0\n",
    "    random.shuffle(train_vectors)\n",
    "    bert_net.train()\n",
    "    \n",
    "    for i in range(0, len(train_vectors), 32):\n",
    "        optimizer.zero_grad()\n",
    "        batch = np.array(train_vectors[i:i+32])\n",
    "        \n",
    "        inp = torch.from_numpy(batch).double()\n",
    "        out, hidden = bert_net(inp)\n",
    "        target_sf = hidden.new_full(hidden[0].size(), fill_value=sparsity_frac)\n",
    "        \n",
    "        loss = mse_criterion(out, inp) + torch.sum(torch.clamp((torch.mean(hidden, axis=0) - target_sf), min=0) ** 2) + torch.mean(hidden * (1 - hidden))\n",
    "        loss.backward()\n",
    "        if steps % 100 == 0:\n",
    "            print('Epoch: {} \\t Step: {} \\t Training Loss: {}'.format(epoch, steps, loss.detach().numpy()))\n",
    "        steps += 1\n",
    "        optimizer.step()\n",
    "        \n",
    "    bert_net.eval()\n",
    "    batch = np.array(test_vectors)\n",
    "    inp = torch.from_numpy(batch).double()\n",
    "    out, hidden = bert_net(inp)\n",
    "    loss = mse_criterion(out, inp)\n",
    "    print('Epoch: {} \\t Testing Reconstruction Loss: {}'.format(epoch, loss.detach().numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bert_net.eval()\n",
    "sparse_bert_vectors = {}\n",
    "\n",
    "for word in frequent_words:\n",
    "    o, h = bert_net(torch.from_numpy(np.array([bert_vectors[word]])).double())\n",
    "    sparse_bert_vectors[word] = h.detach().numpy().squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: science\n",
      "Dimension: 1227\n",
      "Closest words:\n",
      "('business', 1.0)\n",
      "('health', 1.0)\n",
      "('policy', 1.0)\n",
      "('university', 1.0)\n",
      "('management', 1.0)\n",
      "('education', 1.0)\n",
      "('technology', 1.0)\n",
      "('sports', 1.0)\n",
      "('systems', 1.0)\n",
      "('media', 1.0)\n"
     ]
    }
   ],
   "source": [
    "word = 'science'\n",
    "science_dims = np.abs(sparse_bert_vectors[word])\n",
    "print('Word:', word)\n",
    "\n",
    "max_idx = np.argmax(science_dims)\n",
    "print('Dimension:', max_idx)\n",
    "\n",
    "closest_words = []\n",
    "for word in bert_vectors:\n",
    "    closest_words.append((word, abs(sparse_bert_vectors[word][max_idx])))\n",
    "\n",
    "print('Closest words:')    \n",
    "closest_list = sorted(closest_words, key=lambda x: x[1], reverse=True)[:10]\n",
    "for i in closest_list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "torch.save(bert_net.state_dict(), 'sparse_net_bert_1500.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "from transformers import BertModel, BertPreTrainedModel, BertConfig \n",
    "\n",
    "class BertForSequenceClassificationWithSparsity(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.sparse_size = 1000\n",
    "        \n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n",
    "        \n",
    "        self.sparse_net = DenoisingAutoencoder(config.hidden_size, self.sparse_size)\n",
    "        self.sparse_net.load_state_dict(torch.load('sparse_net_bert.pth'))\n",
    "        self.sparsity_frac = 0.05\n",
    "        self.sparsity_imp = 0.1\n",
    "        \n",
    "        self.sparse_dense = nn.Linear(self.sparse_size, self.sparse_size)\n",
    "        self.sparse_activation = nn.Tanh()\n",
    "        self.sparse_classifier = nn.Linear(self.sparse_size, self.config.num_labels)\n",
    "        \n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "        )\n",
    "\n",
    "        osize = outputs[0].size()\n",
    "        all_outputs = outputs[0].reshape(osize[0]*osize[1], self.hidden_size)\n",
    "        rec_outputs, sparse_outputs = self.sparse_net(all_outputs)\n",
    "        \n",
    "#         pooled_output = outputs[1]\n",
    "#         pooled_output = self.dropout(pooled_output)\n",
    "#         logits = self.classifier(pooled_output)\n",
    "\n",
    "        loss_recon = MSELoss()\n",
    "        target_sf = sparse_outputs.new_full(sparse_outputs[0].size(), fill_value=self.sparsity_frac)\n",
    "        loss = self.sparsity_imp * (loss_recon(rec_outputs, all_outputs) + torch.sum(torch.clamp((sparse_outputs.mean(axis=0) - target_sf), min=0) ** 2) + torch.mean(sparse_outputs * (1 - sparse_outputs)))\n",
    "        \n",
    "        sparse_outputs = sparse_outputs.reshape(osize[0], osize[1], -1)\n",
    "        sp_first_token_tensor = sparse_outputs[:, 0]\n",
    "        sp_pooled_output = self.sparse_dense(sp_first_token_tensor)\n",
    "        sp_pooled_output = self.sparse_activation(sp_pooled_output)\n",
    "        sp_pooled_output = self.dropout(sp_pooled_output)\n",
    "        sp_logits = self.sparse_classifier(sp_pooled_output)\n",
    "        \n",
    "        outputs = (sp_logits,) + outputs[2:]  # add hidden states and attention if they are here\n",
    "        \n",
    "        if labels is not None:\n",
    "            if self.num_labels == 1:\n",
    "                #  We are doing regression\n",
    "                loss_fct = MSELoss()\n",
    "                loss += loss_fct(sp_logits.view(-1), labels.view(-1))\n",
    "            else:\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss += loss_fct(sp_logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        outputs = (loss,) + outputs\n",
    "\n",
    "        return outputs  # (loss), logits, (hidden_states), (attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = BertConfig.from_pretrained('bert-base-uncased',\n",
    "                                      num_labels=3,\n",
    "                                      finetuning_task='xnli',\n",
    "                                      cache_dir=None)\n",
    "\n",
    "model = BertForSequenceClassificationWithSparsity.from_pretrained('bert-base-uncased',\n",
    "                                        from_tf=False,\n",
    "                                        config=config,\n",
    "                                        cache_dir=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([tokenizer.encode('this is it'), tokenizer.encode('this is not')])\n",
    "labels = torch.tensor([0, 1])\n",
    "a, b = model(input_ids=input_ids, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseBertConfig(BertConfig):\n",
    "\n",
    "    def __init__(self, **args):\n",
    "        super().__init__(**args)\n",
    "\n",
    "        self.sparsity_frac = None\n",
    "        self.sparsity_imp = None\n",
    "        self.sparse_size = None\n",
    "        self.sparse_noise_stf = None\n",
    "        self.sparse_net_params = None\n",
    "        self.pred_input_size = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = SparseBertConfig.from_pretrained('bert-base-uncased')\n",
    "config.sparsity_frac = .05\n",
    "config.sparsity_imp = .1\n",
    "config.sparse_size = 1500\n",
    "config.sparse_noise_stf = .4\n",
    "config.sparse_net_params = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"architectures\": [\n",
       "    \"BertForMaskedLM\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"finetuning_task\": null,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"is_decoder\": false,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"num_labels\": 2,\n",
       "  \"output_attentions\": false,\n",
       "  \"output_hidden_states\": false,\n",
       "  \"output_past\": true,\n",
       "  \"pred_input_size\": null,\n",
       "  \"pruned_heads\": {},\n",
       "  \"sparse_net_params\": \"\",\n",
       "  \"sparse_noise_stf\": 0.4,\n",
       "  \"sparse_size\": 1500,\n",
       "  \"sparsity_frac\": 0.05,\n",
       "  \"sparsity_imp\": 0.1,\n",
       "  \"torchscript\": false,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"use_bfloat16\": false,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
